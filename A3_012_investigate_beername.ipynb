{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import regex as re\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import NormalPredictor\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "from utilities import regex_utility as reutil\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "print(str(len(stopwords.words(\"english\"))))\n",
    "\n",
    "print(sorted(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details\n",
    "\n",
    "To start with, this is basically just a copy paste of the surprise normal sample from Week 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 -- Recommendation Systems\n",
    "\n",
    "* The final challenge is much like Assignment 2 -- but scoped appropriately for the time and your current abilities.\n",
    "* It is ratings prediction, just like the movielens recommendations we have seen and many other similar problems.\n",
    "* The features created are based on Beer Reviews from experts on a website.\n",
    "* Each beer has been scored between 0 and 5 (on a real scale, so 2.75 or 3.5 is OK).\n",
    "* The official measure is Mean Average Error (MAE) which is pretty intuitive to work with. Everything supports is and it is easy to interpret.\n",
    "* A set of features have been created based on the reviewer, the written review, and information about the Beer being reviewed.\n",
    "* Not all features have to be used, and you can easily create new features using the data if you like.\n",
    "* The features included are:\n",
    "\n",
    "![title](Images/A3Features.png)\n",
    "\n",
    "* Sizes of the files are:\n",
    "|Size | File|\n",
    "|---|---|\n",
    "| 1.9G | features.tsv |\n",
    "| 88B  | header-features.tsv|\n",
    "| 48B  | header.tsv |\n",
    "| 15M  | test.tsv |\n",
    "| 50M  | train.tsv |\n",
    "| 16M |  val.tsv |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_012_investigate beer name\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFilePath = baseDataDir + 'train.tsv'\n",
    "valiFilePath = baseDataDir + 'val.tsv'\n",
    "featuresFilePath = baseDataDir + 'features.tsv'\n",
    "testFilePath = baseDataDir + 'test.tsv'\n",
    "\n",
    "# trainFilePath = baseDataDir + 'train_200k.tsv'\n",
    "# valiFilePath = baseDataDir + 'vali_200k.tsv'\n",
    "# featuresFilePath = baseDataDir + 'features_200k.tsv'\n",
    "# testFilePath = baseDataDir + 'test_200k.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trainFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "df_vali = pd.read_csv(valiFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(testFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[[\"RowID\", \"BeerName\"]]\n",
    "df_vali = df_vali[[\"RowID\", \"BeerName\"]]\n",
    "df_test = df_test[[\"RowID\", \"BeerName\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_train.append(df_vali).append(df_test)\n",
    "\n",
    "del df_train\n",
    "del df_vali\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tinker with NLP preprocessing stuff on Beer Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "stopwordsSorted = sorted(stopwords.words(\"english\"))\n",
    "print(stopwordsSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top(lstTokens, numberToPrint=5):\n",
    "  for i in range(numberToPrint):\n",
    "    print(\"[\" + \",\".join(lstTokens[i]) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"BeerName\"\n",
    "# Remove ascii encoding and punctuation\n",
    "df_combined[colName] = df_combined.apply(lambda x: str(x[colName]).encode(\"ascii\", \"ignore\").decode(), axis=1)\n",
    "df_combined[colName] = df_combined.apply(lambda x: reutil.str_strip_punctuation(str(x[colName])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n"
     ]
    }
   ],
   "source": [
    "# convert target column into a list of word tokens  \n",
    "lstTokens = df_combined.apply(lambda x: str(x[\"BeerName\"]).split(\" \"), axis=1)\n",
    "lstTokens = lstTokens.to_list()\n",
    "\n",
    "\n",
    "\n",
    "# Do Text Preprocessing: \n",
    "# remove capitalisation, single letter tokens and stop words\n",
    "lstTokens = list(map(lambda x: list(map(lambda y: y.lower(), x)), lstTokens))\n",
    "lstTokens = list(map(lambda x: list(filter(lambda y: len(y) >= 2, x)), lstTokens))\n",
    "\n",
    "print_top(lstTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sort each token list so we can use bsearch\n",
    "stopwordsSorted = sorted(stopwords.words(\"english\"))  \n",
    "lstTokens = list(map(lambda x: databasic.filter_by_words_bsearch(x, stopwordsSorted), lstTokens))\n",
    "\n",
    "# Create variables for the words and vocab. When we update out tokens lists, we will want to recompile our word list and vocabulary to keep it up to date\n",
    "words, vocab = databasic.createWordsAndVocabForTokenLists(lstTokens)\n",
    "\n",
    "# Create a term Frequency distribution\n",
    "term_fd = nltk.FreqDist(words)\n",
    "\n",
    "# remove single occurrence words\n",
    "setSingleWords = set(term_fd.hapaxes())\n",
    "lstSingleWords = sorted(list(setSingleWords))\n",
    "lstTokens = list(map(lambda x: databasic.filter_by_words_bsearch(x, lstSingleWords), lstTokens))\n",
    "\n",
    "\n",
    "print_top(lstTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('pale', 'ale'), 80431), (('imperial', 'stout'), 31652), (('india', 'pale'), 31589), (('samuel', 'adams'), 31002), (('sierra', 'nevada'), 20164), (('brown', 'ale'), 19706), (('oatmeal', 'stout'), 14025), (('style', 'ale'), 11846), (('ale', 'samuel'), 11155), (('amber', 'ale'), 11035), (('great', 'lakes'), 10825), (('ale', 'sierra'), 10398), (('ale', 'stone'), 10035), (('double', 'ipa'), 9832), (('samuel', 'smiths'), 9729), (('barley', 'wine'), 8201), (('chocolate', 'stout'), 7904), (('red', 'ale'), 7614), (('winter', 'ale'), 7217), (('christmas', 'ale'), 7153), (('ale', 'bells'), 7136), (('minute', 'ipa'), 6950), (('anniversary', 'ale'), 6887), (('pumpkin', 'ale'), 6698), (('stout', 'bells'), 6568), (('barrel', 'aged'), 6333), (('nut', 'brown'), 6049), (('imperial', 'ipa'), 6028), (('grand', 'cru'), 5970), (('bastard', 'ale'), 5905), (('stout', 'founders'), 5865), (('oak', 'aged'), 5634), (('heavy', 'seas'), 5611), (('stout', 'samuel'), 5572), (('lager', 'samuel'), 5516), (('bourbon', 'barrel'), 5234), (('st', 'bernardus'), 5230), (('ipa', 'india'), 5153), (('dark', 'horse'), 5001), (('new', 'holland'), 4997), (('scotch', 'ale'), 4863), (('white', 'ale'), 4772), (('ale', 'double'), 4742), (('russian', 'imperial'), 4691), (('belgian', 'style'), 4683), (('imperial', 'porter'), 4661), (('stout', 'old'), 4476), (('milk', 'stout'), 4448), (('series', 'smuttynose'), 4438), (('summer', 'ale'), 4416), (('imperial', 'india'), 4412), (('big', 'beer'), 4382), (('beer', 'series'), 4366), (('special', 'ale'), 4306), (('wheat', 'beer'), 4229), (('harvest', 'ale'), 4228), (('trappistes', 'rochefort'), 4222), (('breakfast', 'stout'), 4193), (('ipa', 'stone'), 4162), (('arrogant', 'bastard'), 4108), (('green', 'flash'), 3939), (('ale', 'old'), 3915), (('golden', 'ale'), 3913), (('ale', 'special'), 3886), (('smoked', 'porter'), 3767), (('ale', 'brooklyn'), 3740), (('hop', 'ale'), 3638), (('ale', 'red'), 3509), (('cream', 'stout'), 3457), (('yeti', 'imperial'), 3419), (('barleywine', 'style'), 3412), (('premium', 'lager'), 3385), (('wine', 'style'), 3364), (('otter', 'creek'), 3362), (('old', 'rasputin'), 3360), (('black', 'lager'), 3335), (('adams', 'boston'), 3288), (('stout', 'stone'), 3272), (('wheat', 'ale'), 3161), (('stout', 'bourbon'), 3099), (('irish', 'red'), 3098), (('celebration', 'ale'), 3045), (('coffee', 'stout'), 3036), (('abbey', 'ale'), 3016), (('extra', 'stout'), 2998), (('ale', 'new'), 2988), (('extra', 'pale'), 2987), (('90', 'minute'), 2954), (('ipa', '90'), 2951), (('imperial', 'russian'), 2942), (('beer', 'anchor'), 2942), (('black', 'ale'), 2940), (('russian', 'stout'), 2928), (('ale', 'saranac'), 2897), (('ale', 'heavy'), 2883), (('ale', 'india'), 2875), (('brooklyn', 'black'), 2873), (('anchor', 'christmas'), 2854), (('porter', 'samuel'), 2852), (('blue', 'moon'), 2847)]\n"
     ]
    }
   ],
   "source": [
    "setTopBigrams = 50\n",
    "removeTopFrequentTokens = 50\n",
    "\n",
    "\n",
    "# find the most commong bigrams and join them together to be a single term\n",
    "if setTopBigrams > 0:\n",
    "  bigrams = ngrams(words, n = 2)\n",
    "  fdbigram = nltk.FreqDist(bigrams)\n",
    "  mostFreqBigrams = fdbigram.most_common(setTopBigrams)    \n",
    "  print(mostFreqBigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n"
     ]
    }
   ],
   "source": [
    "lstWorking = lstTokens.copy()\n",
    "print_top(lstWorking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pale_ale', 'imperial_stout', 'india_pale', 'samuel_adams', 'sierra_nevada', 'brown_ale', 'oatmeal_stout', 'style_ale', 'ale_samuel', 'amber_ale', 'great_lakes', 'ale_sierra', 'ale_stone', 'double_ipa', 'samuel_smiths', 'barley_wine', 'chocolate_stout', 'red_ale', 'winter_ale', 'christmas_ale', 'ale_bells', 'minute_ipa', 'anniversary_ale', 'pumpkin_ale', 'stout_bells', 'barrel_aged', 'nut_brown', 'imperial_ipa', 'grand_cru', 'bastard_ale', 'stout_founders', 'oak_aged', 'heavy_seas', 'stout_samuel', 'lager_samuel', 'bourbon_barrel', 'st_bernardus', 'ipa_india', 'dark_horse', 'new_holland', 'scotch_ale', 'white_ale', 'ale_double', 'russian_imperial', 'belgian_style', 'imperial_porter', 'stout_old', 'milk_stout', 'series_smuttynose', 'summer_ale', 'imperial_india', 'big_beer', 'beer_series', 'special_ale', 'wheat_beer', 'harvest_ale', 'trappistes_rochefort', 'breakfast_stout', 'ipa_stone', 'arrogant_bastard', 'green_flash', 'ale_old', 'golden_ale', 'ale_special', 'smoked_porter', 'ale_brooklyn', 'hop_ale', 'ale_red', 'cream_stout', 'yeti_imperial', 'barleywine_style', 'premium_lager', 'wine_style', 'otter_creek', 'old_rasputin', 'black_lager', 'adams_boston', 'stout_stone', 'wheat_ale', 'stout_bourbon', 'irish_red', 'celebration_ale', 'coffee_stout', 'abbey_ale', 'extra_stout', 'ale_new', 'extra_pale', '90_minute', 'ipa_90', 'imperial_russian', 'beer_anchor', 'black_ale', 'russian_stout', 'ale_saranac', 'ale_heavy', 'ale_india', 'brooklyn_black', 'anchor_christmas', 'porter_samuel', 'blue_moon']\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n",
      "[rauch,bock]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if setTopBigrams > 0:\n",
    "  # Convert the bigrams to a list of bigram strings (with spaces)\n",
    "  rep_patterns = list(map(lambda x: x[0], mostFreqBigrams))\n",
    "  rep_patterns = list(map(lambda x: x[0] + \" \" + x[1], rep_patterns))\n",
    "\n",
    "  # Create another list which is the replacements with the\n",
    "  replacements = list(map(lambda x: x.replace(\" \", \"_\"), rep_patterns))\n",
    "  \n",
    "  print(replacements)\n",
    "\n",
    "  lstWorking = [\" \".join(tokens) for tokens in lstWorking] # convert all the token lists back into a single string\n",
    "\n",
    "  # Loop thought and basically find/replace all the bigrams in the string\n",
    "  for i in range(0, len(lstWorking)): \n",
    "      for j in  range(0,len(rep_patterns)):\n",
    "          lstWorking[i] = re.sub(rep_patterns[j], replacements[j], lstWorking[i]) # replace with bigram representation \n",
    "\n",
    "  lstWorking = [tokens.split(\" \") for tokens in lstWorking] # convert back to tokenised lists    \n",
    "\n",
    "\n",
    "print_top(lstWorking)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ale', 351864), ('stout', 125445), ('ipa', 105278), ('pale', 84325), ('imperial', 70272), ('porter', 57977), ('lager', 45827), ('samuel', 40761), ('beer', 38657), ('black', 36881), ('double', 34185), ('india', 33548), ('old', 32536), ('adams', 31939), ('brown', 30044), ('red', 28881), ('hop', 24847), ('stone', 23694), ('wheat', 21158), ('style', 21066), ('amber', 20302), ('sierra', 20265), ('nevada', 20164), ('founders', 18874), ('bells', 18787), ('dark', 18733), ('de', 18689), ('oatmeal', 17744), ('winter', 17622), ('white', 17216), ('series', 16110), ('barrel', 15806), ('anniversary', 15575), ('bock', 15308), ('aged', 15058), ('chocolate', 14353), ('saison', 14221), ('special', 14215), ('st', 13890), ('big', 13796), ('tripel', 13378), ('extra', 13323), ('blue', 12972), ('great', 12726), ('belgian', 11819), ('brooklyn', 11787), ('reserve', 11708), ('pilsner', 11671), ('bourbon', 11540), ('la', 11509)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Look at the most common words, and remove\n",
    "if removeTopFrequentTokens > 0:\n",
    "  setMostFreqWords = term_fd.most_common(removeTopFrequentTokens)\n",
    "  print(setMostFreqWords)\n",
    "  # lstMostFreqWordsKeys = sorted(list(map(lambda x: x[0], setMostFreqWords)))\n",
    "  # lstTokens = list(map(lambda x: databasic.filter_by_words_bsearch(x, lstMostFreqWordsKeys), lstTokens))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
