{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# import pandas as pd\n",
    "import dask.dataframe as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "from utilities import dataframe_utility as dfutil\n",
    "import features_utility as featutil\n",
    "\n",
    "from contentknn import ContentKNNFullCosSimilarityAlgorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details - Light GBM Regression on Beer Context columns inc review counts\n",
    "\n",
    "This is a Candidate for being used in an Ensemble. \n",
    "Characteristicts:\n",
    "* Light GBM Regression Algorithm\n",
    "* Using Beer Context columns inc ABV, Year and Review Counts\n",
    "* Todo: use optimised parameters for Light GBM Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_161_contentknn_tinkering2\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "writeSubRunFile = True\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainFullProcessedPath = baseDataDir + 'train_features_preprocessed.csv'\n",
    "valiFullProcessedPath = baseDataDir + 'vali_features_preprocessed.csv'\n",
    "testFullProcessedPath = baseDataDir + 'test_features_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trainFullProcessedPath)\n",
    "\n",
    "# df_train = df_train.iloc[0:10000, :]\n",
    "# print(df_train.shape)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vali = pd.read_csv(valiFullProcessedPath)\n",
    "\n",
    "# df_vali = df_vali.iloc[0:10000, :]\n",
    "# print(df_vali.shape)\n",
    "# df_vali.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = dd.from_pandas(df_train, npartitions=10)\n",
    "# df_vali = dd.from_pandas(df_vali, npartitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the Review Count columns for Reviewers and Beers to both the Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_train.columns\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainIds = df_train[idCols]\n",
    "dfTrainTarget = df_train[target_col]\n",
    "\n",
    "dfValiIds = df_vali[idCols]\n",
    "dfValiTarget = df_vali[target_col]\n",
    "\n",
    "# This time, we need to keep the Row Id so we can use it for the cosine similarity\n",
    "# feature_cols =  col_names.drop(['BeerID','ReviewerID','rating'])\n",
    "feature_cols = [ \"RowID\" ]\n",
    "feature_cols = feature_cols + list(filter(lambda x: x.startswith(\"BeerType_\"), col_names))\n",
    "dfFullFeatures = df_train[feature_cols]\n",
    "# dfFullFeatures = df_train[feature_cols].append(df_vali[feature_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFullFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "dsetTrainFeatures = Dataset.load_from_df(df_train[['BeerID','ReviewerID',\n",
    "                                    'rating']],reader)\n",
    "\n",
    "dsetValiFeatures = Dataset.load_from_df(df_vali[['BeerID','ReviewerID',\n",
    "                                     'rating']],reader)\n",
    "trainsetTrainFeatures = dsetTrainFeatures.build_full_trainset()\n",
    "\n",
    "print(type(dsetTrainFeatures))\n",
    "print(type(trainsetTrainFeatures))\n",
    "trainsetTrainFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(dsetValiFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NA,valset = train_test_split(dsetValiFeatures, test_size=1.0)\n",
    "\n",
    "# simple Tuning best params: {'bsl_options': }\n",
    "\n",
    "algorithm = ContentKNNFullCosSimilarityAlgorithm()\n",
    "algorithm.setFeatures(dfFullFeatures)\n",
    "\n",
    "# algorithm.fit_simulation(trainsetTrainFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = algorithm.fit(trainsetTrainFeatures)\n",
    "predictions = algorithm.test(valset)\n",
    "\n",
    "\n",
    "# Score our predictions with MAE\n",
    "# It is around 0.77, which means the a random guess based on the distribution of the data\n",
    "# is on average within 0.77 (plus or minus) the true label.\n",
    "# Not bad! You can beat it though, I'm sure :).\n",
    "# Smaller MAE is the better. Good luck!\n",
    "mae = accuracy.mae(predictions,verbose=True)\n",
    "\n",
    "print(\"Average MAE: \" + str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(valset))\n",
    "print(valset[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(predictions))\n",
    "print(str(len(predictions)))\n",
    "print(predictions[0:10])\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Predictions to a dataframe so we can lookup predictions easy\n",
    "lstUIds = list(map(lambda x: x.uid, predictions))\n",
    "lstIIds = list(map(lambda x: x.iid, predictions))\n",
    "lstTrueRatings = list(map(lambda x: x.r_ui, predictions))\n",
    "lstRatingEst = list(map(lambda x: x.est, predictions))\n",
    "\n",
    "\n",
    "# uid == BeerId, iid == ReviewerId, r_ui == Original Ration, est = Predicted rating\n",
    "dfPredictions = pd.DataFrame({ \"uid\": lstUIds,\"iid\": lstIIds, \"r_ui\": lstTrueRatings, \"Predict\": lstRatingEst })\n",
    "\n",
    "dfPredictions.head()\n",
    "# dfPredictions[dfPredictions.uid == 3519]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfValiIds.shape)\n",
    "print(dfPredictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the predictions to the ids, sort by rowid and write to file\n",
    "dfPredictions = pd.merge(dfValiIds, dfPredictions, how=\"inner\", left_on=[\"BeerID\", \"ReviewerID\"], right_on=[\"uid\", \"iid\"])\n",
    "dfPredictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to a subrun file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if writeSubRunFile:\n",
    "  dfPredictions.sort_values(\"RowID\")[[\"RowID\", \"BeerID\", \"ReviewerID\", \"Predict\"]].to_csv(subrunDir + filePrefix + \"_subrun.csv\", index=False)\n",
    "\n",
    "print(\"Average MAE: \" + str(mae))\n",
    "print(\"analyse_maes.append(\" + str(mae) + \")\")\n",
    "print(dfPredictions.shape)\n",
    "dfPredictions.sort_values(\"RowID\").head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "MAE on just 10k records:\n",
    "Average MAE: 0.4534419870925659"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
