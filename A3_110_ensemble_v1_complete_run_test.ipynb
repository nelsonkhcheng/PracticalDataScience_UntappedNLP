{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import accuracy\n",
    "from surprise import dump\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "import features_utility as featutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details\n",
    "\n",
    "Working notebook of a Full Ensemble Run structure, with just KNNWithMeans Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_110_ensemble_v1_complete_run_test\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "runDir = \"runs/\"\n",
    "modelsDir = \"models/\"\n",
    "\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFilePath = baseDataDir + 'train.tsv'\n",
    "valiFilePath = baseDataDir + 'val.tsv'\n",
    "featuresFilePath = baseDataDir + 'features.tsv'\n",
    "testFilePath = baseDataDir + 'test.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Files one by one then delete them after your done, for memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>BeerID</th>\n",
       "      <th>ReviewerID</th>\n",
       "      <th>BeerName</th>\n",
       "      <th>BeerType</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>12300</td>\n",
       "      <td>10635</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>12300</td>\n",
       "      <td>6547</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>12300</td>\n",
       "      <td>9789</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>12300</td>\n",
       "      <td>7372</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>12300</td>\n",
       "      <td>1302</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>12300</td>\n",
       "      <td>704</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>12300</td>\n",
       "      <td>1747</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>12300</td>\n",
       "      <td>9368</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>12300</td>\n",
       "      <td>2568</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>12300</td>\n",
       "      <td>6838</td>\n",
       "      <td>Rauch Ür Bock</td>\n",
       "      <td>Rauchbier</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID  BeerID  ReviewerID       BeerName   BeerType  rating\n",
       "0     19   12300       10635  Rauch Ür Bock  Rauchbier     4.0\n",
       "1     21   12300        6547  Rauch Ür Bock  Rauchbier     4.5\n",
       "2     23   12300        9789  Rauch Ür Bock  Rauchbier     4.5\n",
       "3     24   12300        7372  Rauch Ür Bock  Rauchbier     5.0\n",
       "4     25   12300        1302  Rauch Ür Bock  Rauchbier     4.5\n",
       "5     26   12300         704  Rauch Ür Bock  Rauchbier     4.5\n",
       "6     29   12300        1747  Rauch Ür Bock  Rauchbier     5.0\n",
       "7     31   12300        9368  Rauch Ür Bock  Rauchbier     4.5\n",
       "8     32   12300        2568  Rauch Ür Bock  Rauchbier     4.0\n",
       "9     33   12300        6838  Rauch Ür Bock  Rauchbier     4.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(trainFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "df_train.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data to be just the Reviewer and the Beer(Item) and the Rating Label we want to learn.\n",
    "dfTrainFeatures = df_train.drop(['RowID','BeerName','BeerType'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Stage\n",
    "\n",
    "For the Collaborative Filtering Models, we only need the Training set. Train the models, then save them to file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "dsetTrainFeatures = Dataset.load_from_df(dfTrainFeatures[['BeerID','ReviewerID', 'rating']],reader)\n",
    "trainsetTrainFeatures = dsetTrainFeatures.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSurpriseModel(algorithm, trainset, modelsDir, filePrefix, modelName):\n",
    "  # Train the model then Save the predictor model to file\n",
    "  model = algorithm.fit(trainset)  \n",
    "  dump.dump(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\", None, model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "The dump has been saved as file models/A3_110_ensemble_v1_fullrun_test_knnwithmeans_predictor.model\n",
      "Estimating biases using als...\n",
      "The dump has been saved as file models/A3_110_ensemble_v1_fullrun_test_baselineonly_predictor.model\n"
     ]
    }
   ],
   "source": [
    "# Create each algorithm, train the model, save it to file for later, then delete the model\n",
    "\n",
    "predictorKNN = KNNWithMeans(k=80)\n",
    "trainSurpriseModel(predictorKNN, trainsetTrainFeatures, modelsDir, filePrefix, \"knnwithmeans\")\n",
    "del predictorKNN\n",
    "\n",
    "predictorBaselineOnly = BaselineOnly(bsl_options = {'n_epochs': 8, 'reg_u': 4, 'reg_i': 15})\n",
    "trainSurpriseModel(predictorBaselineOnly, trainsetTrainFeatures, modelsDir, filePrefix, \"baselineonly\")\n",
    "del predictorBaselineOnly\n",
    "\n",
    "# predictorSVDpp = SVDpp(n_factors = 10, n_epochs=20, lr_all=0.005, reg_all=0.2)\n",
    "# trainSurpriseModel(predictorSVDpp, trainsetTrainFeatures, modelsDir, filePrefix, \"svdpp\")\n",
    "# del predictorSVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will want to work on the Content Filter models, todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the training data\n",
    "del trainsetTrainFeatures\n",
    "del reader\n",
    "del dsetTrainFeatures\n",
    "del dfTrainFeatures\n",
    "del df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict On Validation Data with Models Stage\n",
    "\n",
    "Now we want to load the Validation set to we can predict against it and write out the subrun files, which will be used later for the Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full)\n",
    "df_vali = pd.read_csv(valiFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "dfValiIds = df_vali[idCols]\n",
    "dfValiFeatures = df_vali.drop(['RowID','BeerName','BeerType'],axis=1)\n",
    "\n",
    "dsetValiFeatures = Dataset.load_from_df(dfValiFeatures[['BeerID','ReviewerID', 'rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSurpriseModel(modelsDir, filePrefix, modelName, dsName, dataset, dfIds, subrunDir):\n",
    "  # Load the algorithm from the file, the predictions aren't used so that variable will be None\n",
    "  predictions, algorithm = dump.load(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\")\n",
    "  \n",
    "  # Make Predictions using the model\n",
    "  NA,valset = train_test_split(dataset, test_size=1.0)\n",
    "  predictions = algorithm.test(valset)\n",
    "  \n",
    "  # Display the MAE\n",
    "  mae = accuracy.mae(predictions,verbose=True)\n",
    "  print(\"MAE for \" + modelName + \": \" + str(mae))\n",
    "\n",
    "  # Convert the Predictions to a dataframe so we can lookup predictions easy\n",
    "  # uid == BeerId, iid == ReviewerId, r_ui == Original Ration, est = Predicted rating\n",
    "  lstUIds = list(map(lambda x: x.uid, predictions))\n",
    "  lstIIds = list(map(lambda x: x.iid, predictions))\n",
    "  lstTrueRatings = list(map(lambda x: x.r_ui, predictions))\n",
    "  lstRatingEst = list(map(lambda x: x.est, predictions))\n",
    "  dfPredictions = pd.DataFrame({ \"uid\": lstUIds,\"iid\": lstIIds, \"r_ui\": lstTrueRatings, \"Predict\": lstRatingEst })  \n",
    "\n",
    "  # join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "  subRunFilePath = subrunDir + filePrefix + \"_\" + modelName + \"_\" + dsName + \"_subrun.csv\"\n",
    "  dfPredictions = pd.merge(dfIds, dfPredictions, how=\"inner\", left_on=[\"BeerID\", \"ReviewerID\"], right_on=[\"uid\", \"iid\"])\n",
    "  dfPredictions.sort_values(\"RowID\")[[\"RowID\", \"BeerID\", \"ReviewerID\", \"Predict\"]].to_csv(subRunFilePath, index=False)\n",
    "\n",
    "  # Clean up the variables from memory\n",
    "  del predictions\n",
    "  del algorithm\n",
    "  del valset\n",
    "  del lstUIds\n",
    "  del lstIIds\n",
    "  del lstTrueRatings\n",
    "  del lstRatingEst\n",
    "  del dfPredictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.4401\n",
      "MAE for knnwithmeans: 0.4401281860792271\n",
      "MAE:  0.4399\n",
      "MAE for baselineonly: 0.4399004824650012\n"
     ]
    }
   ],
   "source": [
    "predictSurpriseModel(modelsDir, filePrefix, \"knnwithmeans\", \"val\", dsetValiFeatures, dfValiIds, subrunDir)\n",
    "predictSurpriseModel(modelsDir, filePrefix, \"baselineonly\", \"val\", dsetValiFeatures, dfValiIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"svdpp_val\", dsetValiFeatures, dfValiIds, subrunDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables from the Predict Stage\n",
    "del df_vali\n",
    "del reader\n",
    "del dfValiIds\n",
    "del dfValiFeatures\n",
    "del dsetValiFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Ensemble Model\n",
    "\n",
    "Now that all the sub run files have been generated, combine all the predictions into one dataset, train a new final, ensemble model, predict on the validation data and get an MAE and save the model for use later on the Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full) again. But this time, we just want the Row and the rating\n",
    "df_vali = pd.read_csv(valiFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "\n",
    "df_ensemble_full = df_vali[[\"RowID\", \"rating\"]]      \n",
    "\n",
    "del df_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the sub runs and join them together with the ensemble data\n",
    "\n",
    "# Collaborative Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"knnwithmeans\" + \"_val\" + \"_subrun\"\n",
    "df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "fileName = filePrefix + \"_\" + \"baselineonly\" + \"_val\" + \"_subrun\"\n",
    "df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"svdpp\" + \"_val\" + \"_subrun\"\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# # Content Filter Runs\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, \"A3_062_lgbm_regression_beercontext_subrun\")\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, \"A3_063_lgbm_regression_consumercontext_subrun\")\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, \"A3_080_sk_linreg1_subrun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_ensemble_full.columns\n",
    "\n",
    "idCols = ['RowID']\n",
    "feature_cols =  col_names.drop(['RowID','rating'])\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainIds = df_ensemble_full[idCols]\n",
    "dfTrainFeatures = df_ensemble_full[feature_cols]\n",
    "dfTrainTarget = df_ensemble_full[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Final Average MAE (from validation data): 0.4157887110103682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2452e152f70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing the final Ensemble prediction using Light GBM Regression, params tuned\n",
    "\n",
    "# Create the model and predict\n",
    "model = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed,\n",
    "  learning_rate=0.298864877137463, num_leaves=127, max_depth=26, n_estimators=974\n",
    ")\n",
    "model.fit(X=dfTrainFeatures, y=dfTrainTarget)\n",
    "\n",
    "# use the model to predict\n",
    "test_predicted = model.predict(dfTrainFeatures)\n",
    "dfPredicted = pd.DataFrame({\"Predict\": test_predicted})\n",
    "\n",
    "# Calc the MAE and display\n",
    "mae = mean_absolute_error(dfTrainTarget, test_predicted)\n",
    "print(\"Ensemble Final Average MAE (from validation data): \" + str(mae))\n",
    "\n",
    "# Save the model to file\n",
    "model.booster_.save_model(modelsDir + filePrefix + \"_ensemble_predictor.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all the variables\n",
    "del df_ensemble_full\n",
    "del dfTrainIds\n",
    "del dfTrainFeatures\n",
    "del dfTrainTarget\n",
    "del model\n",
    "del test_predicted\n",
    "del dfPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Test data with Models for Subruns\n",
    "\n",
    "Now that we have the final Ensemble model, we can process the Test data\n",
    "\n",
    "First we need to load the test data, and create all the sub runs by using all the base level models to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full)\n",
    "df_test = pd.read_csv(testFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType'])\n",
    "\n",
    "# The test set is unlabeled, so we don't know the true ratings. Populate a rating col with zeros, as we are going\n",
    "# to predict these values\n",
    "df_test[\"rating\"] = 0\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "dfTestIds = df_test[idCols]\n",
    "dfTestFeatures = df_test.drop(['RowID','BeerName','BeerType'],axis=1)\n",
    "\n",
    "dsetTestFeatures = Dataset.load_from_df(dfTestFeatures[['BeerID','ReviewerID','rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3.8224\n",
      "MAE for knnwithmeans: 3.8224290696665966\n",
      "MAE:  3.8284\n",
      "MAE for baselineonly: 3.8284220621788614\n"
     ]
    }
   ],
   "source": [
    "predictSurpriseModel(modelsDir, filePrefix, \"knnwithmeans\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)\n",
    "predictSurpriseModel(modelsDir, filePrefix, \"baselineonly\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"svdpp\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables from the Predict Stage\n",
    "del reader\n",
    "del dfTestIds\n",
    "del dfTestFeatures\n",
    "del dsetTestFeatures\n",
    "\n",
    "# Keep this, as we will use this in the next stage\n",
    "# del df_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ensemble Model and predict on the Test data\n",
    "\n",
    "Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble_test = df_test[[\"RowID\"]]      \n",
    "\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the sub runs and join them together with the ensemble data\n",
    "\n",
    "# Collaborative Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"knnwithmeans\" + \"_test\" + \"_subrun\"\n",
    "df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "fileName = filePrefix + \"_\" + \"baselineonly\" + \"_test\" + \"_subrun\"\n",
    "df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"svdpp\" + \"_test\" + \"_subrun\"\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# # Content Filter Runs\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, \"A3_062_lgbm_regression_beercontext_subrun\")\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, \"A3_063_lgbm_regression_consumercontext_subrun\")\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, \"A3_080_sk_linreg1_subrun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowID</th>\n",
       "      <th>A3_110_ensemble_v1_fullrun_test_knnwithmeans_test_subrun</th>\n",
       "      <th>A3_110_ensemble_v1_fullrun_test_baselineonly_test_subrun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4.054958</td>\n",
       "      <td>4.066056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>4.144754</td>\n",
       "      <td>4.092544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>4.328965</td>\n",
       "      <td>4.285232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>4.056218</td>\n",
       "      <td>4.114596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>4.357345</td>\n",
       "      <td>4.367196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowID  A3_110_ensemble_v1_fullrun_test_knnwithmeans_test_subrun  \\\n",
       "0     18                                           4.054958          \n",
       "1     20                                           4.144754          \n",
       "2     30                                           4.328965          \n",
       "3     46                                           4.056218          \n",
       "4     47                                           4.357345          \n",
       "\n",
       "   A3_110_ensemble_v1_fullrun_test_baselineonly_test_subrun  \n",
       "0                                           4.066056         \n",
       "1                                           4.092544         \n",
       "2                                           4.285232         \n",
       "3                                           4.114596         \n",
       "4                                           4.367196         "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ensemble_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_ensemble_test.columns\n",
    "\n",
    "idCols = ['RowID']\n",
    "feature_cols =  col_names.drop(['RowID'])\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTestFeatures = df_ensemble_test[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nelso\\AppData\\Local\\Temp/ipykernel_4164/316437830.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfPredictions[\"Score\"] = predicted\n"
     ]
    }
   ],
   "source": [
    "# load the ensemble model  and predict\n",
    "model = lgb.Booster(model_file=modelsDir + filePrefix + \"_ensemble_predictor.model\")\n",
    "predicted = model.predict(dfTestFeatures)\n",
    "\n",
    "dfPredictions = df_ensemble_test[idCols]\n",
    "dfPredictions[\"Score\"] = predicted\n",
    "\n",
    "# join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "finalRunFilePath = runDir + filePrefix + \"_run.tsv\"\n",
    "dfPredictions.to_csv(finalRunFilePath, sep=\"\\t\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables\n",
    "del df_ensemble_test\n",
    "del dfTestFeatures\n",
    "del model\n",
    "del predicted\n",
    "del dfPredictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
