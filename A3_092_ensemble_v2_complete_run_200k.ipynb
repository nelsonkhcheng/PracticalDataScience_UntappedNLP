{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import accuracy\n",
    "from surprise import dump\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "from utilities import dataframe_utility as dfutil\n",
    "import features_utility as featutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details\n",
    "\n",
    "Working notebook of a Full Ensemble Run structure, with just KNNWithMeans Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_092_ensemble_v2_complete_run_200k\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "runDir = \"runs/\"\n",
    "modelsDir = \"models/\"\n",
    "featuresDataDir = \"features/\"\n",
    "\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFilePath = baseDataDir + 'train.tsv'\n",
    "# valiFilePath = baseDataDir + 'val.tsv'\n",
    "# featuresFilePath = baseDataDir + 'features.tsv'\n",
    "# testFilePath = baseDataDir + 'test.tsv'\n",
    "\n",
    "trainFilePath = baseDataDir + 'train_200k.tsv'\n",
    "valiFilePath = baseDataDir + 'vali_200k.tsv'\n",
    "featuresFilePath = baseDataDir + 'features_200k.tsv'\n",
    "testFilePath = baseDataDir + 'test_200k.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Files one by one then delete them after your done, for memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trainFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the data to be just the Reviewer and the Beer(Item) and the Rating Label we want to learn.\n",
    "dfTrainFeatures = df_train.drop(['RowID','BeerName','BeerType'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filter Models: Train\n",
    "\n",
    "For the Collaborative Filtering Models, we only need the Training set. Train the models, then save them to file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into a Surprise dataset\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "dsetTrainFeatures = Dataset.load_from_df(dfTrainFeatures[['BeerID','ReviewerID', 'rating']],reader)\n",
    "trainsetTrainFeatures = dsetTrainFeatures.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSurpriseModel(algorithm, trainset, modelsDir, filePrefix, modelName):\n",
    "  # Train the model then Save the predictor model to file\n",
    "  model = algorithm.fit(trainset)  \n",
    "  dump.dump(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\", None, model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create each algorithm, train the model, save it to file for later, then delete the model\n",
    "\n",
    "predictorKNN = KNNWithMeans(k=80)\n",
    "trainSurpriseModel(predictorKNN, trainsetTrainFeatures, modelsDir, filePrefix, \"knnwithmeans\")\n",
    "del predictorKNN\n",
    "\n",
    "# predictorBaselineOnly = BaselineOnly(bsl_options = {'n_epochs': 8, 'reg_u': 4, 'reg_i': 15})\n",
    "# trainSurpriseModel(predictorBaselineOnly, trainsetTrainFeatures, modelsDir, filePrefix, \"baselineonly\")\n",
    "# del predictorBaselineOnly\n",
    "\n",
    "# predictorSVDpp = SVDpp(n_factors = 10, n_epochs=20, lr_all=0.005, reg_all=0.2)\n",
    "# trainSurpriseModel(predictorSVDpp, trainsetTrainFeatures, modelsDir, filePrefix, \"svdpp\")\n",
    "# del predictorSVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the training data used for the collaborate filters\n",
    "del trainsetTrainFeatures\n",
    "del reader\n",
    "del dsetTrainFeatures\n",
    "del dfTrainFeatures\n",
    "del df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filter Models: Predict On Validation Data \n",
    "\n",
    "Now we want to load the Validation set to we can predict against it and write out the subrun files, which will be used later for the Ensemble.\n",
    "\n",
    "First, do the Predictions for the Collaborative Filter models (surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full)\n",
    "df_vali = pd.read_csv(valiFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "dfValiIds = df_vali[idCols]\n",
    "dfValiFeatures = df_vali.drop(['RowID','BeerName','BeerType'],axis=1)\n",
    "\n",
    "dsetValiFeatures = Dataset.load_from_df(dfValiFeatures[['BeerID','ReviewerID', 'rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSurpriseModel(modelsDir, filePrefix, modelName, dsName, dataset, dfIds, subrunDir):\n",
    "  # Load the algorithm from the file, the predictions aren't used so that variable will be None\n",
    "  predictions, algorithm = dump.load(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\")\n",
    "  \n",
    "  # Make Predictions using the model\n",
    "  NA,valset = train_test_split(dataset, test_size=1.0)\n",
    "  predictions = algorithm.test(valset)\n",
    "  \n",
    "  # Display the MAE\n",
    "  mae = accuracy.mae(predictions,verbose=True)\n",
    "  print(\"MAE for \" + modelName + \": \" + str(mae))\n",
    "\n",
    "  # Convert the Predictions to a dataframe so we can lookup predictions easy\n",
    "  # uid == BeerId, iid == ReviewerId, r_ui == Original Ration, est = Predicted rating\n",
    "  lstUIds = list(map(lambda x: x.uid, predictions))\n",
    "  lstIIds = list(map(lambda x: x.iid, predictions))\n",
    "  lstTrueRatings = list(map(lambda x: x.r_ui, predictions))\n",
    "  lstRatingEst = list(map(lambda x: x.est, predictions))\n",
    "  dfPredictions = pd.DataFrame({ \"uid\": lstUIds,\"iid\": lstIIds, \"r_ui\": lstTrueRatings, \"Predict\": lstRatingEst })  \n",
    "\n",
    "  # join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "  subRunFilePath = subrunDir + filePrefix + \"_\" + modelName + \"_\" + dsName + \"_subrun.csv\"\n",
    "  dfPredictions = pd.merge(dfIds, dfPredictions, how=\"inner\", left_on=[\"BeerID\", \"ReviewerID\"], right_on=[\"uid\", \"iid\"])\n",
    "  dfPredictions.sort_values(\"RowID\")[[\"RowID\", \"BeerID\", \"ReviewerID\", \"Predict\"]].to_csv(subRunFilePath, index=False)\n",
    "\n",
    "  # Clean up the variables from memory\n",
    "  del predictions\n",
    "  del algorithm\n",
    "  del valset\n",
    "  del lstUIds\n",
    "  del lstIIds\n",
    "  del lstTrueRatings\n",
    "  del lstRatingEst\n",
    "  del dfPredictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSurpriseModel(modelsDir, filePrefix, \"knnwithmeans\", \"val\", dsetValiFeatures, dfValiIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"baselineonly\", \"val\", dsetValiFeatures, dfValiIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"svdpp\", \"val\", dsetValiFeatures, dfValiIds, subrunDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables from the Predict Stage\n",
    "del df_vali\n",
    "del reader\n",
    "del dfValiIds\n",
    "del dfValiFeatures\n",
    "del dsetValiFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Filter Models, train and predict\n",
    "\n",
    "First we want to load the features and do all the data preprocessing, then we can train the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "df_train = pd.read_csv(trainFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "\n",
    "# Load the validation data. When we want to do one hot encoding, we have to do it over both datasets to ensure consistency\n",
    "df_vali = pd.read_csv(valiFilePath,sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID', 'BeerName','BeerType','rating'])\n",
    "\n",
    "# Load the validation data. When we want to do one hot encoding, we have to do it over both datasets to ensure consistency\n",
    "df_test = pd.read_csv(testFilePath,sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID', 'BeerName','BeerType','rating'])                         \n",
    "\n",
    "# Load the features\n",
    "df_features = pd.read_csv(featuresFilePath,sep='\\t',\n",
    "    names=['RowID','BrewerID','ABV','DayofWeek','Month',\n",
    "          'DayofMonth','Year','TimeOfDay','Gender',\n",
    "          'Birthday','Text','Lemmatized','POS_Tag'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the reviewer counts to each of the data sets\n",
    "df_train = featutil.addReviewerReviewCount(df_train)\n",
    "df_train = featutil.addBeerReviewCount(df_train)\n",
    "\n",
    "df_vali = featutil.addReviewerReviewCount(df_vali)\n",
    "df_vali = featutil.addBeerReviewCount(df_vali)\n",
    "\n",
    "df_test = featutil.addReviewerReviewCount(df_test)\n",
    "df_test = featutil.addBeerReviewCount(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToUse = [\"RowID\", \"BrewerID\", \"ABV\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"Year\", \"Gender\", \"TimeOfDay\"]\n",
    "\n",
    "df_train_data = df_train.join(df_features[colsToUse], on=\"RowID\", how=\"inner\", rsuffix=\"Feat\")\n",
    "df_vali_data = df_vali.join(df_features[colsToUse], on=\"RowID\", how=\"inner\", rsuffix=\"Feat\")\n",
    "df_test_data = df_test.join(df_features[colsToUse], on=\"RowID\", how=\"inner\", rsuffix=\"Feat\")\n",
    "\n",
    "# Remove the duplicated Row ID, also remove Beer Name at this point, we're nt using it\n",
    "df_train_data = df_train_data.drop(['RowIDFeat', \"BeerName\"],axis=1)\n",
    "df_vali_data = df_vali_data.drop(['RowIDFeat', \"BeerName\"],axis=1)\n",
    "df_test_data = df_test_data.drop(['RowIDFeat', \"BeerName\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up these dataframes now that they have been joined\n",
    "del df_train\n",
    "del df_vali\n",
    "del df_features\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the feature transformations\n",
    "df_train_data = featutil.fixNullABV(df_train_data)\n",
    "df_vali_data = featutil.fixNullABV(df_vali_data)\n",
    "df_test_data = featutil.fixNullABV(df_test_data)\n",
    "\n",
    "df_train_data, df_vali_data, df_test_data = dfutil.getDummiesForTripleSets(df_train_data, df_vali_data, df_test_data, \"BrewerID\")\n",
    "\n",
    "df_train_data, df_vali_data, df_test_data = dfutil.getDummiesForTripleSets(df_train_data, df_vali_data, df_test_data, \"BeerType\")\n",
    "\n",
    "df_train_data, df_vali_data, df_test_data = dfutil.getDummiesForTripleSets(df_train_data, df_vali_data, df_test_data, \"Gender\")\n",
    "\n",
    "df_train_data = featutil.formatDayOfWeek(df_train_data)\n",
    "df_vali_data = featutil.formatDayOfWeek(df_vali_data)\n",
    "df_test_data = featutil.formatDayOfWeek(df_test_data)\n",
    "\n",
    "df_train_data = featutil.formatMonth(df_train_data)\n",
    "df_vali_data = featutil.formatMonth(df_vali_data)\n",
    "df_test_data = featutil.formatMonth(df_test_data)\n",
    "\n",
    "df_train_data = featutil.formatTimeToSec(df_train_data)\n",
    "df_vali_data = featutil.formatTimeToSec(df_vali_data)\n",
    "df_test_data = featutil.formatTimeToSec(df_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_data.shape)\n",
    "print(df_vali_data.shape)\n",
    "print(df_test_data.shape)\n",
    "df_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the test data file out so we can load it back in later so as not to have to redo this step\n",
    "df_test_data.to_csv(featuresDataDir + filePrefix + \"_test_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_train_data.columns\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "feature_cols =  col_names.drop(['RowID','BeerID','ReviewerID','rating' ])\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainFeatures = df_train_data[feature_cols]\n",
    "dfTrainTarget = df_train_data[target_col]\n",
    "\n",
    "dfValiIds = df_vali_data[idCols]\n",
    "dfValiFeatures = df_vali_data[feature_cols]\n",
    "dfValiTarget = df_vali_data[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLightGbmModel(model, train_feat, train_target, \n",
    "      modelsDir, filePrefix, modelName):\n",
    "\n",
    "  # Train the model and Save the predictor model to file\n",
    "  model.fit(X=train_feat, y=train_target) \n",
    "  model.booster_.save_model(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\")\n",
    "\n",
    "\n",
    "def trainSkLinearRegModel(model, \n",
    "      train_feat, train_target, vali_ids, vali_feat, vali_target, \n",
    "      modelsDir, filePrefix, dsName, modelName):\n",
    "  # Train the model then Save the predictor model to file\n",
    "  model.fit(train_feat, train_target) \n",
    "  pickle.dump(model, open(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLightGbmModel(vali_ids, vali_feat, vali_target, \n",
    "      modelsDir, filePrefix, dsName, modelName):\n",
    "\n",
    "  model = lgb.Booster(model_file=modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\")\n",
    "\n",
    "  predicted = model.predict(vali_feat)\n",
    "  dfPredicted = pd.DataFrame({\"Predict\": predicted})\n",
    "\n",
    "  # join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "  subRunFilePath = subrunDir + filePrefix + \"_\" + modelName + \"_\" + dsName + \"_subrun.csv\"\n",
    "  dfPredicted = pd.concat([vali_ids.reset_index(), dfPredicted], axis=1).drop(columns=\"index\")\n",
    "  dfPredicted.to_csv(subRunFilePath, index=False)\n",
    "\n",
    "  if vali_target is not None:      \n",
    "    mae = mean_absolute_error(vali_target, predicted)\n",
    "    print(\"MAE for \" + modelName + \": \" + str(mae))\n",
    "\n",
    "  # clean up variables in memory\n",
    "  del predicted\n",
    "  del dfPredicted\n",
    "\n",
    "\n",
    "def predictSkLinearRegModel(vali_ids, vali_feat, vali_target, \n",
    "      modelsDir, filePrefix, dsName, modelName):\n",
    "\n",
    "  model = pickle.load(open(modelsDir + filePrefix + \"_\" + modelName + \"_predictor.model\", 'rb'))\n",
    "\n",
    "  predicted = model.predict(vali_feat)\n",
    "  dfPredicted = pd.DataFrame({\"Predict\": predicted})\n",
    "\n",
    "  # join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "  subRunFilePath = subrunDir + filePrefix + \"_\" + modelName + \"_\" + dsName + \"_subrun.csv\"\n",
    "  dfPredicted = pd.concat([vali_ids.reset_index(), dfPredicted], axis=1).drop(columns=\"index\")\n",
    "  dfPredicted.to_csv(subRunFilePath, index=False)\n",
    "\n",
    "  # When used on the test data, we have not target/label data. This is just used for evaluation, to \n",
    "  # calculate our MAE against real labels. If none is passed in, don't do this\n",
    "  if vali_target is not None:      \n",
    "    mae = mean_absolute_error(vali_target, predicted)\n",
    "    print(\"MAE for \" + modelName + \": \" + str(mae))\n",
    "\n",
    "  # clean up variables in memory\n",
    "  del predicted\n",
    "  del dfPredicted  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesBeerContext(df1):\n",
    "  consumerCols = [\"DayofWeek\", \"DayofMonth\", \"Month\", \"TimeOfDay\", \"Gender_Male\", \"Gender_Female\", \"Gender_unknown\"]\n",
    "  return dfutil.getFeaturesWithoutCols(df1, consumerCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models, save them to file and then clear the model from memory\n",
    "modelBeerContext = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed\n",
    "    ,learning_rate=0.010443500090385492, num_leaves = 68, max_depth = 14, n_estimators = 608\n",
    "  )  \n",
    "dfTrainFeatures_BeerContext = getFeaturesBeerContext(dfTrainFeatures)\n",
    "dfValiFeatures_BeerContext = getFeaturesBeerContext(dfValiFeatures)\n",
    "trainLightGbmModel(modelBeerContext, dfTrainFeatures_BeerContext, dfTrainTarget, \n",
    "    modelsDir, filePrefix, \"lgbm_beercontext\")\n",
    "predictLightGbmModel(dfValiIds, dfValiFeatures_BeerContext, dfValiTarget,\n",
    "    modelsDir, filePrefix, \"val\", \"lgbm_beercontext\")    \n",
    "del dfTrainFeatures_BeerContext\n",
    "del dfValiFeatures_BeerContext\n",
    "del modelBeerContext\n",
    "\n",
    "\n",
    "# modelConsumerContext = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed\n",
    "#     ,learning_rate=0.26879548049242075, num_leaves = 91, max_depth = 2, n_estimators = 384\n",
    "#  ) \n",
    "# dfTrainFeatures_ConsumerContext = dfTrainFeatures[[\"Year\", \"ReviewerReviewCount\", \"BeerReviewCount\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"TimeOfDay\", \"Gender_Male\", \"Gender_Female\", \"Gender_unknown\"]]\n",
    "# dfValiFeatures_ConsumerContext = dfValiFeatures[[\"Year\", \"ReviewerReviewCount\", \"BeerReviewCount\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"TimeOfDay\", \"Gender_Male\", \"Gender_Female\", \"Gender_unknown\"]]\n",
    "# trainLightGbmModel(modelConsumerContext, dfTrainFeatures_ConsumerContext, dfTrainTarget, \n",
    "#     modelsDir, filePrefix, \"lgbm_consumercontext\")\n",
    "# predictLightGbmModel(dfValiIds, dfValiFeatures_ConsumerContext, dfValiTarget,\n",
    "#     modelsDir, filePrefix, \"val\", \"lgbm_consumercontext\")    \n",
    "# del dfTrainFeatures_ConsumerContext\n",
    "# del dfValiFeatures_ConsumerContext\n",
    "# del modelConsumerContext\n",
    "\n",
    "\n",
    "# modelLinReg = LinearRegression()\n",
    "# trainSkLinearRegModel(modelLinReg, dfTrainFeatures, dfTrainTarget, \n",
    "#     modelsDir, filePrefix, \"val\", \"sklinearreg\")\n",
    "# predictSkLinearRegModel(dfValiIds, dfValiFeatures, dfValiTarget,\n",
    "#     modelsDir, filePrefix, \"val\", \"sklinearreg\")\n",
    "# del modelLinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the variables from memory\n",
    "del df_train_data\n",
    "del df_vali_data\n",
    "del df_test_data\n",
    "del dfTrainFeatures\n",
    "del dfTrainTarget\n",
    "del dfValiIds\n",
    "del dfValiFeatures\n",
    "del dfValiTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Ensemble Model\n",
    "\n",
    "Now that all the sub run files have been generated, combine all the predictions into one dataset, train a new final, ensemble model, predict on the validation data and get an MAE and save the model for use later on the Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full) again. But this time, we just want the Row and the rating\n",
    "df_vali = pd.read_csv(valiFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType','rating'])\n",
    "\n",
    "df_ensemble_full = df_vali[[\"RowID\", \"rating\"]]      \n",
    "\n",
    "del df_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the sub runs and join them together with the ensemble data\n",
    "\n",
    "# Collaborative Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"knnwithmeans\" + \"_val\" + \"_subrun\"\n",
    "df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"baselineonly\" + \"_val\" + \"_subrun\"\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"svdpp\" + \"_val\" + \"_subrun\"\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# # Content Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"lgbm_beercontext\" + \"_val\" + \"_subrun\"\n",
    "df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"lgbm_consumercontext\" + \"_val\" + \"_subrun\"\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"sklinearreg\" + \"_val\" + \"_subrun\"\n",
    "# df_ensemble_full = featutil.joinRunToEnsembleFrame(df_ensemble_full, subrunDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_ensemble_full.columns\n",
    "\n",
    "idCols = ['RowID']\n",
    "feature_cols =  col_names.drop(['RowID','rating'])\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainIds = df_ensemble_full[idCols]\n",
    "dfTrainFeatures = df_ensemble_full[feature_cols]\n",
    "dfTrainTarget = df_ensemble_full[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the final Ensemble prediction using Light GBM Regression, params tuned\n",
    "\n",
    "# Create the model and predict\n",
    "model = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed,\n",
    "  learning_rate=0.298864877137463, num_leaves=127, max_depth=26, n_estimators=974\n",
    ")\n",
    "model.fit(X=dfTrainFeatures, y=dfTrainTarget)\n",
    "\n",
    "# use the model to predict\n",
    "test_predicted = model.predict(dfTrainFeatures)\n",
    "dfPredicted = pd.DataFrame({\"Predict\": test_predicted})\n",
    "\n",
    "# Calc the MAE and display\n",
    "mae = mean_absolute_error(dfTrainTarget, test_predicted)\n",
    "print(\"Ensemble Final Average MAE (from validation data): \" + str(mae))\n",
    "\n",
    "# Save the model to file\n",
    "model.booster_.save_model(modelsDir + filePrefix + \"_ensemble_predictor.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up all the variables\n",
    "del df_ensemble_full\n",
    "del dfTrainIds\n",
    "del dfTrainFeatures\n",
    "del dfTrainTarget\n",
    "del model\n",
    "del test_predicted\n",
    "del dfPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the Test data with Models for Subruns\n",
    "\n",
    "Now that we have the final Ensemble model, we can process the Test data. We need to load the test data, and create all the sub runs by using all the base level models to predict.\n",
    "\n",
    "First, predict using the Collaborative Filter Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the validation data (in full)\n",
    "df_test = pd.read_csv(testFilePath,sep='\\t',\n",
    "              names=['RowID','BeerID','ReviewerID','BeerName','BeerType'])\n",
    "\n",
    "# The test set is unlabeled, so we don't know the true ratings. Populate a rating col with zeros, as we are going\n",
    "# to predict these values\n",
    "df_test[\"rating\"] = 0\n",
    "\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "dfTestIds = df_test[idCols]\n",
    "dfTestFeatures = df_test.drop(['RowID','BeerName','BeerType'],axis=1)\n",
    "\n",
    "dsetTestFeatures = Dataset.load_from_df(dfTestFeatures[['BeerID','ReviewerID','rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the Collaborative Filter Models\n",
    "predictSurpriseModel(modelsDir, filePrefix, \"knnwithmeans\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"baselineonly\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)\n",
    "# predictSurpriseModel(modelsDir, filePrefix, \"svdpp\", \"test\", dsetTestFeatures, dfTestIds, subrunDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables from the Predict Stage\n",
    "del reader\n",
    "del dfTestIds\n",
    "del dfTestFeatures\n",
    "del dsetTestFeatures\n",
    "\n",
    "# Keep this, as we will use this in the next stage\n",
    "# del df_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Predict using the Content Filter Models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload that test data that was cleaned and processed previously\n",
    "df_test_data = pd.read_csv(featuresDataDir + filePrefix + \"_test_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colsToUse = [\"RowID\", \"BrewerID\", \"ABV\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"Year\", \"Gender\", \"TimeOfDay\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_test_data.columns\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "feature_cols =  col_names.drop(['RowID','BeerID','ReviewerID', 'rating' ])\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTestIds = df_test_data[idCols]\n",
    "dfTestFeatures = df_test_data[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test_data.columns)\n",
    "df_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem is with one hot encoding, different sets of brewers or beer types between the training data (train+vali) and what is in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can make predictions according to each of our Content Filter Models. Pass None for the target set, the function\n",
    "# will just skip the evaluation (calculating the MAE)\n",
    "dfTestFeatures_BeerContext =  getFeaturesBeerContext(dfTestFeatures)\n",
    "predictLightGbmModel(dfTestIds, dfTestFeatures_BeerContext, None,\n",
    "    modelsDir, filePrefix, \"test\", \"lgbm_beercontext\")    \n",
    "del dfTestFeatures_BeerContext\n",
    "\n",
    "\n",
    "# dfTestFeatures_ConsumerContext = dfTestFeatures[[\"Year\", \"ReviewerReviewCount\", \"BeerReviewCount\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"TimeOfDay\", \"Gender_Male\", \"Gender_Female\", \"Gender_unknown\"]]\n",
    "# predictLightGbmModel(dfTestIds, dfTestFeatures_ConsumerContext, None,\n",
    "#     modelsDir, filePrefix, \"test\", \"lgbm_consumercontext\") \n",
    "# del dfTestFeatures_ConsumerContext   \n",
    "\n",
    "\n",
    "# modelLinReg = LinearRegression()\n",
    "# predictSkLinearRegModel(dfTestIds, dfValiFeatures, None,\n",
    "#     modelsDir, filePrefix, \"test\", \"sklinearreg\")\n",
    "# del modelLinReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Ensemble Model and predict on the Test data\n",
    "\n",
    "Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble_test = df_test[[\"RowID\"]]      \n",
    "\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the sub runs and join them together with the ensemble data\n",
    "\n",
    "# Collaborative Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"knnwithmeans\" + \"_test\" + \"_subrun\"\n",
    "df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"baselineonly\" + \"_test\" + \"_subrun\"\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"svdpp\" + \"_test\" + \"_subrun\"\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# # Content Filter Runs\n",
    "fileName = filePrefix + \"_\" + \"lgbm_beercontext\" + \"_test\" + \"_subrun\"\n",
    "df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"lgbm_consumercontext\" + \"_test\" + \"_subrun\"\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)\n",
    "\n",
    "# fileName = filePrefix + \"_\" + \"sklinearreg\" + \"_test\" + \"_subrun\"\n",
    "# df_ensemble_test = featutil.joinRunToEnsembleFrame(df_ensemble_test, subrunDir, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_ensemble_test.columns\n",
    "\n",
    "idCols = ['RowID']\n",
    "feature_cols =  col_names.drop(['RowID'])\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTestFeatures = df_ensemble_test[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ensemble model  and predict\n",
    "model = lgb.Booster(model_file=modelsDir + filePrefix + \"_ensemble_predictor.model\")\n",
    "predicted = model.predict(dfTestFeatures)\n",
    "\n",
    "dfPredictions = df_ensemble_test[idCols]\n",
    "dfPredictions[\"Score\"] = predicted\n",
    "\n",
    "# join the predictions to the ids, sort by rowid and write to out the subrun file\n",
    "finalRunFilePath = runDir + filePrefix + \"_run.tsv\"\n",
    "dfPredictions.to_csv(finalRunFilePath, sep=\"\\t\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up variables\n",
    "del df_ensemble_test\n",
    "del dfTestFeatures\n",
    "del model\n",
    "del predicted\n",
    "del dfPredictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
