{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import fasttext as ft\n",
    "import optuna\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "from utilities import dataframe_utility as dfutil\n",
    "from utilities import regex_utility as reutil\n",
    "import features_utility as featutil\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details - Light GBM Regression NLP\n",
    "\n",
    "First File on looking at doing NLP on the Beer name and text features. Investigate using fast text, because fast!\n",
    "This file might start off with just the beer name, then we will go from there\n",
    "\n",
    "Characteristics:\n",
    "* Light GBM Regression Algorithm\n",
    "* Start working on NLP on the Beer name text columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_122_lgbm_nlp_beername_tuning\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "featuresDataDir = \"features/\"\n",
    "modelsDir = \"models/\"\n",
    "writeSubRunFile = True\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainFilePath = baseDataDir + 'train.tsv'\n",
    "# valiFilePath = baseDataDir + 'val.tsv'\n",
    "# featuresFilePath = baseDataDir + 'features.tsv'\n",
    "# testFilePath = baseDataDir + 'test.tsv'\n",
    "\n",
    "trainFilePath = baseDataDir + 'train_200k.tsv'\n",
    "valiFilePath = baseDataDir + 'vali_200k.tsv'\n",
    "featuresFilePath = baseDataDir + 'features_200k.tsv'\n",
    "testFilePath = baseDataDir + 'test_200k.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trainFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "df_vali = pd.read_csv(valiFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(testFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])                                \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just look at Beer name first. Compile a full list of the beer names, save it to file with one per line. Then we can load it with fasttext and build a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"BeerName\"\n",
    "df_train, df_vali, df_test, documentFilePath = featutil.formatTextColForNLP(df_train, df_vali, df_test, colName, featuresDataDir, filePrefix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a Fast Text language model. Check to see if there is a saved model to use, else train a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in an existing model file to use that was generated from another run. Or leave this empty to automatically\n",
    "# use a language model file specific to this run notebook name\n",
    "modelFileToUse = \"\"\n",
    "\n",
    "fasttext_model = featutil.getFastTextLangModel(colName, modelFileToUse,  modelsDir, filePrefix, documentFilePath, 200, True)\n",
    "\n",
    "print(fasttext_model.words[0:50])\n",
    "\n",
    "# examine some of the word vectors\n",
    "# print(fasttext_model.get_word_vector(\"stout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = df_train\n",
    "df_vali_data = df_vali\n",
    "df_test_data = df_test\n",
    "\n",
    "print(df_train_data.shape)\n",
    "print(df_vali_data.shape)\n",
    "\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that just the Ids, rating and document vectors, but at columns\n",
    "df_train_data = featutil.convertToDocVectorDataSet(df_train_data, colName, fasttext_model)\n",
    "df_vali_data = featutil.convertToDocVectorDataSet(df_vali_data, colName, fasttext_model)\n",
    "df_test_data = featutil.convertToDocVectorDataSet(df_test_data, colName, fasttext_model)\n",
    "\n",
    "df_vali_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test data to file, when we do a complete run. Otherwise, just drop the test data out of memory\n",
    "del df_test\n",
    "del df_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_train_data.columns\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "feature_cols =  col_names.drop(['RowID','BeerID','ReviewerID','rating' ])\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainIds = df_train_data[idCols]\n",
    "dfTrainFeatures = df_train_data[feature_cols]\n",
    "dfTrainTarget = df_train_data[target_col]\n",
    "\n",
    "dfValiIds = df_vali_data[idCols]\n",
    "dfValiFeatures = df_vali_data[feature_cols]\n",
    "dfValiTarget = df_vali_data[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValiIds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrainFeatures.shape)\n",
    "dfTrainFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  # Create the Light GBM Regression model and train\n",
    "  model = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed\n",
    "    ,learning_rate=trial.suggest_float(\"learning_rate\", 0.005, 0.3)\n",
    "    ,num_leaves=trial.suggest_int(\"num_leaves\", 2, 127)\n",
    "    ,max_depth=trial.suggest_int(\"max_depth\", 2, 30)\n",
    "    ,n_estimators=trial.suggest_int(\"n_estimators \", 50, 1000)\n",
    "    # ,min_split_gain=trial.suggest_float(\"min_split_gain\", 0.001, 1.0)\n",
    "    # ,min_child_samples=trial.suggest_int(\"min_child_samples\", 1, 100)  \n",
    "    # #,min_child_weight =trial.suggest_float(\"min_child_weight\", 0.0001, 0.1) \n",
    "    # ,subsample =trial.suggest_float(\"subsample\", 0.1, 1.0) \n",
    "    # ,subsample_freq =trial.suggest_int(\"subsample_freq\", 0, 15)\n",
    "    # ,colsample_bytree =trial.suggest_float(\"colsample_bytree\", 0.1, 1.0) \n",
    "    # ,reg_alpha =trial.suggest_float(\"reg_alpha\", 0.1, 1.0) \n",
    "    # ,reg_lambda =trial.suggest_float(\"reg_lambda\", 0.1, 1.0)      \n",
    "  )\n",
    "\n",
    "  model.fit(X=dfTrainFeatures, y=dfTrainTarget)\n",
    "\n",
    "  # Use the model to predict against our validation data\n",
    "  test_predicted = model.predict(dfValiFeatures)  \n",
    "\n",
    "  mae = mean_absolute_error(dfValiTarget, test_predicted)\n",
    "\n",
    "  return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"\\n---------\")\n",
    "print(\"Study Complete\")\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_params)\n",
    "print(\"Best Rank Score: \" + str(study.best_value))\n",
    "print(\"-------\")\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Run 1 - 50 ish trial\n",
    "* Beer Name NLP, Full Data\n",
    "* parameters: {'learning_rate': 0.2472480229316372, 'num_leaves': 125, 'max_depth': 24, 'n_estimators ': 796}. \n",
    "* Best is trial 27 with value: 0.4170567352847134.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
