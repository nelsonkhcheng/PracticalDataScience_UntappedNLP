{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import fasttext as ft\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from utilities import data_basic_utility as databasic\n",
    "from utilities import dataframe_utility as dfutil\n",
    "from utilities import regex_utility as reutil\n",
    "import features_utility as featutil\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Details - Light GBM Regression NLP\n",
    "\n",
    "First File on looking at doing NLP on the Beer name and text features. Investigate using fast text, because fast!\n",
    "This file might start off with just the beer name, then we will go from there\n",
    "\n",
    "Characteristics:\n",
    "* Light GBM Regression Algorithm\n",
    "* Start working on NLP on the Beer name text columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePrefix = \"A3_121_lgbm_nlp_beername\"\n",
    "baseDataDir = \"C:/Development/Data/COSC2670/Assignment3/A3data/\"\n",
    "subrunDir = \"subruns/\"\n",
    "featuresDataDir = \"features/\"\n",
    "modelsDir = \"models/\"\n",
    "writeSubRunFile = True\n",
    "seed = databasic.get_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFilePath = baseDataDir + 'train.tsv'\n",
    "valiFilePath = baseDataDir + 'val.tsv'\n",
    "featuresFilePath = baseDataDir + 'features.tsv'\n",
    "testFilePath = baseDataDir + 'test.tsv'\n",
    "\n",
    "# trainFilePath = baseDataDir + 'train_200k.tsv'\n",
    "# valiFilePath = baseDataDir + 'vali_200k.tsv'\n",
    "# featuresFilePath = baseDataDir + 'features_200k.tsv'\n",
    "# testFilePath = baseDataDir + 'test_200k.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trainFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "df_vali = pd.read_csv(valiFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(testFilePath, sep='\\t',\n",
    "                         names=['RowID','BeerID','ReviewerID',\n",
    "                                  'BeerName','BeerType','rating'])                                \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RowID BrewerID ABV DayofWeek Month DayofMonth Year TimeOfDay Gender Birthday Text Lemmatized POS_Tag\n",
    "# # df_features = pd.read_csv(baseDataDir + 'features_500k.tsv',sep='\\t', names=['RowID','BrewerID','ABV','DayofWeek','Month',\n",
    "# df_features = pd.read_csv(baseDataDir + 'features_200k.tsv',sep='\\t', names=['RowID','BrewerID','ABV','DayofWeek','Month',\n",
    "#                                                                  'DayofMonth','Year','TimeOfDay','Gender',\n",
    "#                                                                  'Birthday','Text','Lemmatized','POS_Tag'])\n",
    "\n",
    "# df_features.head()\n",
    "\n",
    "# colsToUse = [\"RowID\", \"BrewerID\", \"ABV\", \"DayofWeek\", \"DayofMonth\", \"Month\", \"Year\", \"Gender\", \"TimeOfDay\"]\n",
    "\n",
    "# # Find the feature records that match the training and validation data and join them together\n",
    "# dfFullData = df_train.join(df_features[colsToUse], on=\"RowID\", how=\"inner\", rsuffix=\"Feat\")\n",
    "# dfFullDataVali = df_vali.join(df_features[colsToUse], on=\"RowID\", how=\"inner\", rsuffix=\"Feat\")\n",
    "\n",
    "# dfFullData.head()\n",
    "\n",
    "# # Remove the duplicated Row ID, also remove Beer Name at this point, we're nt using it\n",
    "# df_train_data = dfFullData.drop(['RowIDFeat', \"BeerName\"],axis=1)\n",
    "# df_vali_data = dfFullDataVali.drop(['RowIDFeat', \"BeerName\"],axis=1)\n",
    "\n",
    "# df_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just look at Beer name first. Compile a full list of the beer names, save it to file with one per line. Then we can load it with fasttext and build a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"BeerName\"\n",
    "df_train, df_vali, df_test, documentFilePath = featutil.formatTextColForNLP(df_train, df_vali, df_test, colName, featuresDataDir, filePrefix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a Fast Text language model. Check to see if there is a saved model to use, else train a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in an existing model file to use that was generated from another run\n",
    "modelFileToUse = \"\"\n",
    "\n",
    "fasttext_model = featutil.getFastTextLangModel(colName, modelFileToUse,  modelsDir, filePrefix, documentFilePath, 200, True)\n",
    "\n",
    "print(fasttext_model.words[0:50])\n",
    "\n",
    "# examine some of the word vectors\n",
    "# print(fasttext_model.get_word_vector(\"stout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = df_train\n",
    "df_vali_data = df_vali\n",
    "df_test_data = df_test\n",
    "\n",
    "print(df_train_data.shape)\n",
    "print(df_vali_data.shape)\n",
    "\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe that just the Ids, rating and document vectors, but at columns\n",
    "df_train_data = featutil.convertToDocVectorDataSet(df_train_data, colName, fasttext_model)\n",
    "df_vali_data = featutil.convertToDocVectorDataSet(df_vali_data, colName, fasttext_model)\n",
    "df_test_data = featutil.convertToDocVectorDataSet(df_test_data, colName, fasttext_model)\n",
    "\n",
    "df_vali_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test data to file, when we do a complete run. Otherwise, just drop the test data out of memory\n",
    "del df_test\n",
    "del df_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns\n",
    "col_names = df_train_data.columns\n",
    "\n",
    "idCols = ['RowID','BeerID','ReviewerID']\n",
    "feature_cols =  col_names.drop(['RowID','BeerID','ReviewerID','rating' ])\n",
    "target_col = 'rating'\n",
    "\n",
    "# Create the sub data sets of the features and the target\n",
    "dfTrainIds = df_train_data[idCols]\n",
    "dfTrainFeatures = df_train_data[feature_cols]\n",
    "dfTrainTarget = df_train_data[target_col]\n",
    "\n",
    "dfValiIds = df_vali_data[idCols]\n",
    "dfValiFeatures = df_vali_data[feature_cols]\n",
    "dfValiTarget = df_vali_data[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValiIds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrainFeatures.shape)\n",
    "dfTrainFeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  best params\n",
    "# {'learning_rate': 0.2472480229316372, 'num_leaves': 125, 'max_depth': 24, 'n_estimators ': 796}. \n",
    "\n",
    "\n",
    "# No tuning\n",
    "# model = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed)\n",
    "\n",
    "# best params  \n",
    "  \n",
    "model = lgb.LGBMRegressor(objective=\"regression_l1\", metric=\"mae\", random_state=seed\n",
    "    ,learning_rate=0.2472480229316372, num_leaves = 125, max_depth = 24, n_estimators = 796\n",
    "  )\n",
    "\n",
    "model.fit(X=dfTrainFeatures, y=dfTrainTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict against our validation data\n",
    "test_predicted = model.predict(dfValiFeatures)\n",
    "test_predicted[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredicted = pd.DataFrame({\"Predict\": test_predicted})\n",
    "dfPredicted['Predict'].hist(bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(dfValiTarget, test_predicted)\n",
    "\n",
    "print(\"Average MAE: \" + str(mae))\n",
    "print(\"analyse_maes.append(\" + str(mae) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to a subrun file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredicted = pd.concat([dfValiIds.reset_index(), dfPredicted], axis=1).drop(columns=\"index\")\n",
    "\n",
    "if writeSubRunFile:\n",
    "  dfPredicted.to_csv(subrunDir + filePrefix + \"_subrun.csv\", index=False)\n",
    "\n",
    "print(\"Average MAE: \" + str(mae))\n",
    "print(dfPredicted.shape)\n",
    "dfPredicted.sort_values(\"RowID\").head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "No Param Tuning\n",
    "* Beer Name NLP, 200k\n",
    "* MAE 0.4382028648981029\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c06c75df55f9518a2e4db6ce3b8ca21fb7e457d427684d07afebc061061d6a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
